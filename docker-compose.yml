
services:
  # Next.js Frontend + API
  web:
    build: 
      context: ./web
      dockerfile: Dockerfile.dev
    ports:
      - "3001:3000"
    environment:
      - NODE_ENV=development
      - DATABASE_URL=postgresql://personalai:personalai123@db:5432/personalai
      - REDIS_URL=redis://redis:6379
      - NEXTAUTH_URL=https://echosofme.io
      - NEXTAUTH_SECRET=f29c73c05bc1ea75e0afacb0f85478af4443659cdb6bd6bb83f81330a226dbb0
      - ML_INFERENCE_URL=http://ml-inference:8000
      - SMTP_USER=your-gmail@gmail.com
      - SMTP_APP_PASSWORD=your-gmail-app-password
    volumes:
      - ./web:/app
      - /app/node_modules
      - /app/.next
    depends_on:
      - db
      - redis
    command: npm run dev

  # PostgreSQL Database
  db:
    image: postgres:15-alpine
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_DB=personalai
      - POSTGRES_USER=personalai
      - POSTGRES_PASSWORD=personalai123
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init.sql:/docker-entrypoint-initdb.d/init.sql

  # Redis for caching/sessions
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  # pgAdmin for database management
  pgadmin:
    image: dpage/pgadmin4:latest
    ports:
      - "5050:80"
    environment:
      - PGADMIN_DEFAULT_EMAIL=admin@personalai.com
      - PGADMIN_DEFAULT_PASSWORD=admin123
    depends_on:
      - db

  # Python ML Training Container with GPU support
  ml-trainer:
    build:
      context: ./ml
      dockerfile: Dockerfile
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - CUDA_VISIBLE_DEVICES=0
      - DATABASE_URL=postgresql://personalai:personalai123@db:5432/personalai
      - HF_HOME=/models/huggingface
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
    volumes:
      - ./ml:/workspace
      - ./models:/models
      - ./data:/data
      - ./web:/web
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    stdin_open: true
    tty: true
    command: /bin/bash

  # Ollama Service for Mistral-7B
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
    volumes:
      - ./models/ollama:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  # ML Inference Server
  ml-inference:
    image: nvcr.io/nvidia/pytorch:25.04-py3  # RTX 5090 compatible
    runtime: nvidia
    ports:
      - "8000:8000"
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - CUDA_VISIBLE_DEVICES=0
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
      - HF_HOME=/models/huggingface
      - OLLAMA_URL=http://ollama:11434
      - COQUI_TOS_AGREED=1
    volumes:
      - ./models:/models
      - ./ml:/workspace
      - ./web:/web
    working_dir: /workspace
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      - db
      - ollama
    restart: unless-stopped
    command: /workspace/start_rtx5090_voice_service.sh

  # Cloudflare Tunnel for echosofme.io
  tunnel:
    image: cloudflare/cloudflared:latest
    command: tunnel --no-autoupdate --protocol http2 run --token eyJhIjoiN2MzMTI2NzdiZjUzYzMwYmJjZjkwNWVjMWFlZGZkMDIiLCJ0IjoiN2ZiNWY3NDMtOTFjNS00Mzk2LThhZGMtYjBlYzU5NjA0NDU5IiwicyI6Ik1UZ3hNR0ZpTkdNdFpqRmpNQzAwTnpjNUxXRTFOekl0WlRsa1pHRXdaVGMwWkdWayJ9
    restart: on-failure:3
    networks:
      - default
      - web_echosofme_network
    external_links:
      - echosofme_app:web
    environment:
      - TUNNEL_METRICS=0.0.0.0:20241

volumes:
  postgres_data:
  redis_data:

networks:
  web_echosofme_network:
    external: true